# =========================================================
# ğŸ”’ OpenFHE CKKSã‚’ä½¿ç”¨ã—ãŸã‚»ã‚­ãƒ¥ã‚¢é€£åˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
# =========================================================
#
# ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®æ¦‚è¦ã€‘
# ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€CIFAR-10ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ä»¥ä¸‹2ã¤ã®é€£åˆå­¦ç¿’ã‚’æ¯”è¼ƒã—ã¾ã™ï¼š
#   1. å¹³æ–‡ã§ã®é€£åˆå­¦ç¿’ï¼ˆPlain Federated Learningï¼‰
#   2. CKKSæº–åŒå‹æš—å·ã‚’ä½¿ç”¨ã—ãŸé€£åˆå­¦ç¿’ï¼ˆEncrypted Federated Learningï¼‰
#
# ã€ä¸»è¦ãªç‰¹å¾´ã€‘
#   - OpenFHE CKKSãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸæº–åŒå‹æš—å·åŒ–
#   - å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’æš—å·åŒ–ã—ãŸã¾ã¾é›†ç´„å¯èƒ½
#   - CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿéš›ã®ç”»åƒåˆ†é¡ã‚’å®Ÿè¡Œ
#   - å¹³æ–‡ã¨æš—å·åŒ–ã®ç²¾åº¦ãƒ»å®Ÿè¡Œæ™‚é–“ã‚’æ¯”è¼ƒ
#
# ã€å‡¦ç†ã®æµã‚Œã€‘
#   1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé–“ã§ã®åˆ†å‰²
#   2. å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
#   3. ã‚µãƒ¼ãƒãŒå„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é›†ç´„ï¼ˆå¹³æ–‡ or æš—å·åŒ–ï¼‰
#   4. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡
#   5. çµæœã®å¯è¦–åŒ–ã¨ã‚µãƒãƒªãƒ¼å‡ºåŠ›
#
# =========================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import threading
import time
import copy
from math import log2, ceil
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUIãªã—ç’°å¢ƒå¯¾å¿œ
import argparse
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# [OPENFHE-CKKS] è¿½åŠ 
from openfhe import *


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =========================
def _next_pow2(n: int) -> int:
    """
    ä¸ãˆã‚‰ã‚ŒãŸæ•°å€¤nä»¥ä¸Šã®æœ€å°ã®2ã®ç´¯ä¹—ã‚’è¿”ã™
    CKKSæš—å·åŒ–ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯2ã®ç´¯ä¹—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ä½¿ç”¨

    ä¾‹: n=100 â†’ 128, n=1000 â†’ 1024
    """
    return 1 << ceil(log2(max(1, n)))


# =========================================================
# ğŸ”’ CKKSæº–åŒå‹æš—å·ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«é›†ç´„ã‚¯ãƒ©ã‚¹
# =========================================================
# ã€å½¹å‰²ã€‘
# è¤‡æ•°ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰é€ã‚‰ã‚Œã¦ããŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã€æš—å·åŒ–ã—ãŸã¾ã¾å¹³å‡åŒ–ã™ã‚‹
#
# ã€å‡¦ç†ãƒ•ãƒ­ãƒ¼ã€‘
#   1. ã‚µãƒ¼ãƒå´ã§æš—å·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨éµãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆåˆæœŸåŒ–æ™‚ï¼‰
#   2. å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«æš—å·åŒ–
#   3. æš—å·åŒ–ã•ã‚ŒãŸã¾ã¾åŠ ç®—ï¼ˆEvalAddï¼‰
#   4. å¹³å‡åŒ–ã®ãŸã‚ 1/N ã‚’ä¹—ç®—ï¼ˆEvalMultï¼‰
#   5. å¾©å·ã—ã¦å¹³å‡åŒ–ã•ã‚ŒãŸé‡ã¿ã‚’å–å¾—
#
# ã€CKKSæš—å·ã®ç‰¹å¾´ã€‘
#   - æµ®å‹•å°æ•°ç‚¹æ•°ã®è¿‘ä¼¼è¨ˆç®—ãŒå¯èƒ½ãªæº–åŒå‹æš—å·æ–¹å¼
#   - æš—å·åŒ–ã•ã‚ŒãŸã¾ã¾åŠ ç®—ãƒ»ä¹—ç®—ãŒå¯èƒ½
#   - ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šè¤‡æ•°ã®å€¤ã‚’ä¸€ã¤ã®æš—å·æ–‡ã§æ‰±ãˆã‚‹
# =========================================================
class FHEModelAggregator:
    """
    å½¹å‰²:
      - å„ãƒ¬ã‚¤ãƒ¤é‡ã¿ã®å½¢çŠ¶ã‚’è¨˜éŒ²
      - CKKSã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ 1 ã¤æº–å‚™ï¼ˆBatchSize ã¯æœ€å¤§è¦ç´ æ•°ã«åˆã‚ã›ã‚‹ï¼‰
      - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’ãƒ¬ã‚¤ãƒ¤ã”ã¨ã« flatten â†’ CKKS ã§ pack & encrypt
      - ã‚µãƒ¼ãƒå´ã§æš—å·åŠ ç®— & å®šæ•°ä¹—ç®—ï¼ˆ1/num_clientsï¼‰â†’ å¾©å· â†’ å½¢çŠ¶ã«æˆ»ã—ã¦è¿”ã™
    """

    def __init__(
        self,
        model_structure,
        num_clients=5,
        # ä»¥ä¸‹2ã¤ã¯ Concrete-ML ç‰ˆã®æ®‹ç½®å¼•æ•°ï¼ˆå¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ç¶­æŒã®ãŸã‚å—ã‘å–ã‚‹ãŒç„¡è¦–ï¼‰
        scale_factor=100,
        max_value=50,
        # CKKS æš—å·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        mult_depth: int = 1,           # ä¹—ç®—æ·±åº¦ï¼ˆåŠ ç®—ã¨å®šæ•°ä¹—ç®—ã®ã¿ãªã®ã§1ã§ååˆ†ï¼‰
        scale_mod_size: int = 50,      # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã®ãƒ“ãƒƒãƒˆæ•°ï¼ˆç²¾åº¦ã«å½±éŸ¿ï¼‰
        security_level: SecurityLevel = SecurityLevel.HEStd_128_classic,  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«
    ):
        self.num_clients = num_clients

        # ========================================
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®è§£æ
        # ========================================
        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ï¼‰ã®å½¢çŠ¶ã‚’å–å¾—
        # ä¾‹: conv1.weight â†’ (32, 3, 3, 3) ã®ã‚ˆã†ãªå½¢çŠ¶æƒ…å ±ã‚’ä¿å­˜
        self.weight_shapes = {}
        self.buffer_names = set()
        max_elems = 0  # æœ€å¤§è¦ç´ æ•°ã‚’è¨˜éŒ²ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®šã«ä½¿ç”¨ï¼‰

        for name, param in model_structure.named_parameters():
            self.weight_shapes[name] = param.shape
            max_elems = max(max_elems, int(np.prod(param.shape)))

        # ãƒãƒƒãƒ•ã‚¡ï¼ˆBatchNormã®çµ±è¨ˆé‡ãªã©ï¼‰ã¯æš—å·åŒ–ã›ãšå¹³æ–‡ã§æ‰±ã†
        # running_mean, running_var, num_batches_tracked ãªã©ãŒè©²å½“
        for name, buffer in model_structure.named_buffers():
            self.buffer_names.add(name)

        # ========================================
        # ã‚¹ãƒ†ãƒƒãƒ—2: CKKSæš—å·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        # ========================================
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯2ã®ç´¯ä¹—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆCKKSæš—å·ã®ä»•æ§˜ï¼‰
        # ã¾ãŸã€ãƒªãƒ³ã‚°ã‚µã‚¤ã‚ºã®åŠåˆ†ä»¥ä¸‹ã«åˆ¶é™ã•ã‚Œã‚‹
        desired_batch_size = _next_pow2(max_elems)

        # CKKSæš—å·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        params = CCParamsCKKSRNS()
        params.SetMultiplicativeDepth(mult_depth)     # ä¹—ç®—ã®æ·±ã•ï¼ˆå’Œã¨å®šæ•°ä¹—ç®—ã®ã¿ãªã®ã§1ï¼‰
        params.SetScalingModSize(scale_mod_size)      # ç²¾åº¦ã‚’æ±ºå®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params.SetSecurityLevel(security_level)        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«ï¼ˆ128ãƒ“ãƒƒãƒˆï¼‰

        # ä¸€æ™‚çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãƒªãƒ³ã‚°ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        temp_cc = GenCryptoContext(params)
        ring_dim = temp_cc.GetRingDimension()         # ãƒªãƒ³ã‚°å¤šé …å¼ã®æ¬¡å…ƒ
        max_batch_size = ring_dim // 2                 # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ä¸Šé™

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åˆ¶ç´„å†…ã«èª¿æ•´
        self.batch_size = min(desired_batch_size, max_batch_size)
        print(f"ğŸ” CKKS parameters: desired_batch_size={desired_batch_size}, ring_dim={ring_dim}, max_batch_size={max_batch_size}")
        print(f"ğŸ” Using batch_size={self.batch_size}")

        # ========================================
        # ã‚¹ãƒ†ãƒƒãƒ—3: æš—å·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨éµã®ç”Ÿæˆ
        # ========================================
        # å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã§æš—å·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        params.SetBatchSize(self.batch_size)
        self.cc = GenCryptoContext(params)

        # å¿…è¦ãªæš—å·æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        self.cc.Enable(PKESchemeFeature.PKE)           # å…¬é–‹éµæš—å·åŒ–
        self.cc.Enable(PKESchemeFeature.KEYSWITCH)     # éµåˆ‡ã‚Šæ›¿ãˆ
        self.cc.Enable(PKESchemeFeature.LEVELEDSHE)    # ãƒ¬ãƒ™ãƒ«ä»˜ãæº–åŒå‹æš—å·

        # å…¬é–‹éµã¨ç§˜å¯†éµã®ãƒšã‚¢ã‚’ç”Ÿæˆ
        self.keys = self.cc.KeyGen()
        # ä¹—ç®—ç”¨ã®è©•ä¾¡éµã‚’ç”Ÿæˆï¼ˆå¹³æ–‡å®šæ•°ã¨ã®ä¹—ç®—ã«ä½¿ç”¨ï¼‰
        self.cc.EvalMultKeyGen(self.keys.secretKey)

        print(f"ğŸ” CKKS ready: ring dimension = {self.cc.GetRingDimension()}, batch_size = {self.batch_size}")

        # ä»¥å‰ã® Concrete-ML å›è·¯ç½®ãå ´ã¯ãƒ€ãƒŸãƒ¼ã§æ®‹ã™ï¼ˆå¤–éƒ¨å‚ç…§ã•ã‚Œãªã„ãŒäº’æ›ã®ãŸã‚ï¼‰
        self.circuits = {}

    # [REPLACED] Concrete-ML ã®å›è·¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¯å‰Šé™¤ï¼ˆäº’æ›ã®ãŸã‚ç©ºãƒ¡ã‚½ãƒƒãƒ‰ã¯æ®‹ã•ãªã„ï¼‰

    def _split_into_chunks(self, tensor: torch.Tensor, chunk_size: int):
        """
        ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆCKKSæš—å·åŒ–ã®ãŸã‚ï¼‰

        å¤§ããªãƒ†ãƒ³ã‚½ãƒ«ã¯ä¸€ã¤ã®æš—å·æ–‡ã«åã¾ã‚‰ãªã„ãŸã‚ã€
        batch_sizeå˜ä½ã§åˆ†å‰²ã—ã¦è¤‡æ•°ã®æš—å·æ–‡ã¨ã—ã¦æ‰±ã†

        Args:
            tensor: åˆ†å‰²ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ï¼‰
            chunk_size: 1ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®è¦ç´ æ•°ï¼ˆ=batch_sizeï¼‰

        Returns:
            chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¯0ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        """
        # ãƒ†ãƒ³ã‚½ãƒ«ã‚’1æ¬¡å…ƒé…åˆ—ã«å¹³å¦åŒ–ã—ã¦ãƒªã‚¹ãƒˆã«å¤‰æ›
        flat = tensor.detach().cpu().float().numpy().reshape(-1).tolist()
        chunks = []

        # chunk_sizeã”ã¨ã«åˆ†å‰²
        for i in range(0, len(flat), chunk_size):
            chunk = flat[i:i+chunk_size]
            # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ãŒchunk_sizeã‚ˆã‚Šå°ã•ã„å ´åˆã¯0ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(chunk) < chunk_size:
                chunk = chunk + [0.0] * (chunk_size - len(chunk))
            chunks.append(chunk)
        return chunks

    def encrypt_model_weights(self, model_weights_dict):
        """
        ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’CKKSæš—å·åŒ–ã™ã‚‹

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. å„ãƒ¬ã‚¤ãƒ¤ã®é‡ã¿ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        2. å„ãƒãƒ£ãƒ³ã‚¯ã‚’CKKSå¹³æ–‡ï¼ˆPlaintextï¼‰ã«å¤‰æ›
        3. å…¬é–‹éµã§æš—å·åŒ–ã—ã¦æš—å·æ–‡ï¼ˆCiphertextï¼‰ã‚’ç”Ÿæˆ

        Args:
            model_weights_dict: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¾æ›¸ {layer_name: tensor}

        Returns:
            encrypted_weights: æš—å·åŒ–ã•ã‚ŒãŸé‡ã¿ {layer_name: [Ciphertext, ...]}
                              å¤§ããªãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯è¤‡æ•°ã®æš—å·æ–‡ã«åˆ†å‰²ã•ã‚Œã‚‹
        """
        encrypted_weights = {}

        # ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«å‡¦ç†
        for layer_name, shape in self.weight_shapes.items():
            w_tensor: torch.Tensor = model_weights_dict[layer_name]

            # ãƒ†ãƒ³ã‚½ãƒ«ã‚’batch_sizeå˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self._split_into_chunks(w_tensor, self.batch_size)

            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’æš—å·åŒ–
            cts = []
            for chunk in chunks:
                # CKKSå¹³æ–‡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆè¤‡æ•°ã®å€¤ã‚’ãƒ‘ãƒƒã‚¯ï¼‰
                pt = self.cc.MakeCKKSPackedPlaintext(chunk)
                # å…¬é–‹éµã§æš—å·åŒ–
                ct = self.cc.Encrypt(self.keys.publicKey, pt)
                cts.append(ct)

            encrypted_weights[layer_name] = cts

        return encrypted_weights

    def aggregate_encrypted_models(self, client_weights_list):
        """
        è¤‡æ•°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’æš—å·åŒ–ã—ãŸã¾ã¾å¹³å‡åŒ–ã™ã‚‹ï¼ˆã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ï¼‰

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’æš—å·åŒ–
        2. ãƒ¬ã‚¤ãƒ¤ã”ã¨ã€ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«æš—å·æ–‡ã‚’åŠ ç®—ï¼ˆEvalAddï¼‰
        3. 1/N ã‚’ä¹—ç®—ã—ã¦å¹³å‡åŒ–ï¼ˆEvalMultï¼‰
        4. å¾©å·ã—ã¦å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã«æˆ»ã™

        ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
        - æš—å·æ–‡ã®ã¾ã¾åŠ ç®—ãƒ»ä¹—ç®—ã‚’è¡Œã†ãŸã‚ã€ã‚µãƒ¼ãƒã¯å¹³æ–‡ã®é‡ã¿ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ããªã„
        - ã“ã‚Œã«ã‚ˆã‚Šã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãŒä¿è­·ã•ã‚Œã‚‹

        Args:
            client_weights_list: å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®state_dictã®ãƒªã‚¹ãƒˆ

        Returns:
            aggregated_weights: å¹³å‡åŒ–ã•ã‚ŒãŸstate_dictï¼ˆPyTorchãƒ†ãƒ³ã‚½ãƒ«ï¼‰
        """
        print("\nğŸ”’ Starting CKKS model aggregation...")

        # ========================================
        # ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’æš—å·åŒ–
        # ========================================
        encrypted_weights_list = []
        for i, client_state_dict in enumerate(client_weights_list):
            print(f"  ğŸ”‘ Encrypting weights from Client {i+1}...")
            enc = self.encrypt_model_weights(client_state_dict)
            encrypted_weights_list.append(enc)

        # ========================================
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¬ã‚¤ãƒ¤ã”ã¨ã«æš—å·åŒ–ã•ã‚ŒãŸã¾ã¾é›†ç´„
        # ========================================
        aggregated_weights = {}
        inv_n = 1.0 / len(encrypted_weights_list)  # å¹³å‡åŒ–ã®ãŸã‚ã®ä¿‚æ•° (1/N)

        # ãƒãƒƒãƒ•ã‚¡ï¼ˆBatchNormã®çµ±è¨ˆé‡ãªã©ï¼‰ã¯æš—å·åŒ–ã›ãšã€æœ€åˆã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚‚ã®ã‚’ä½¿ç”¨
        for buffer_name in self.buffer_names:
            if buffer_name in client_weights_list[0]:
                aggregated_weights[buffer_name] = client_weights_list[0][buffer_name].clone()

        # å„ãƒ¬ã‚¤ãƒ¤ã®é‡ã¿ã‚’é›†ç´„
        for layer_name, shape in self.weight_shapes.items():
            # å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒã˜ãƒ¬ã‚¤ãƒ¤ã®æš—å·æ–‡ãƒªã‚¹ãƒˆã‚’åé›†
            layer_cts_list = [enc[layer_name] for enc in encrypted_weights_list]
            num_chunks = len(layer_cts_list[0])  # ã“ã®ãƒ¬ã‚¤ãƒ¤ã®ãƒãƒ£ãƒ³ã‚¯æ•°

            # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«é›†ç´„å‡¦ç†
            all_vals = []
            for chunk_idx in range(num_chunks):
                # å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒã˜ãƒãƒ£ãƒ³ã‚¯ã®æš—å·æ–‡ã‚’åé›†
                chunk_cts = [client_cts[chunk_idx] for client_cts in layer_cts_list]

                # ========== æº–åŒå‹åŠ ç®— ==========
                # æš—å·æ–‡ã®ã¾ã¾åŠ ç®—ï¼ˆNå€‹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ã‚’åˆè¨ˆï¼‰
                c_sum = chunk_cts[0]
                for ct in chunk_cts[1:]:
                    c_sum = self.cc.EvalAdd(c_sum, ct)

                # ========== æº–åŒå‹ä¹—ç®— ==========
                # åˆè¨ˆã‚’å¹³å‡åŒ–ã™ã‚‹ãŸã‚ã€å¹³æ–‡å®šæ•° 1/N ã‚’ä¹—ç®—
                # ã“ã®æ“ä½œã‚‚æš—å·æ–‡ã®ã¾ã¾å®Ÿè¡Œã•ã‚Œã‚‹
                c_avg = self.cc.EvalMult(c_sum, inv_n)

                # ========== å¾©å· ==========
                # ç§˜å¯†éµã§å¾©å·ã—ã¦å¹³æ–‡ã®å¹³å‡å€¤ã‚’å–å¾—
                pt_avg = self.cc.Decrypt(c_avg, self.keys.secretKey)
                pt_avg.SetLength(self.batch_size)
                vals = pt_avg.GetRealPackedValue()  # ãƒ‘ãƒƒã‚¯ã•ã‚ŒãŸå€¤ã‚’å±•é–‹
                all_vals.extend(vals)

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»ã—ã¦å…ƒã®å½¢çŠ¶ã«å¾©å…ƒ
            num_elems = int(np.prod(shape))
            trimmed = np.array(all_vals[:num_elems], dtype=np.float32).reshape(shape)
            aggregated_weights[layer_name] = torch.from_numpy(trimmed)

            print(f"  âœ… Aggregated {layer_name} ({num_chunks} chunks) with {len(layer_cts_list)} clients")

        print("âœ… CKKS model aggregation completed!")
        return aggregated_weights


# =========================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆCIFAR-10ç”»åƒåˆ†é¡ï¼‰
# =========================================================

from torchvision import datasets, transforms
from torch.utils.data import Subset
import random


def build_transforms():
    """
    CIFAR-10ç”¨ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆå‰å‡¦ç†ï¼‰ã‚’ä½œæˆ

    ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€‘
    - RandomHorizontalFlip: å·¦å³åè»¢ã§ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
    - RandomCrop: ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ—ã§ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
    - ToTensor: PILç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    - Normalize: å¹³å‡ã¨æ¨™æº–åå·®ã§æ­£è¦åŒ–ï¼ˆCIFAR-10ã®çµ±è¨ˆå€¤ã‚’ä½¿ç”¨ï¼‰

    ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€‘
    - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—ã€æ­£è¦åŒ–ã®ã¿
    """
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),  # 50%ã®ç¢ºç‡ã§å·¦å³åè»¢
        transforms.RandomCrop(32, padding=4),  # 4ãƒ”ã‚¯ã‚»ãƒ«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œã€32x32ã«ã‚¯ãƒ­ãƒƒãƒ—
        transforms.ToTensor(),  # [0,255]ã®ç”»åƒã‚’[0,1]ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # CIFAR-10ã®çµ±è¨ˆå€¤
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    return train_transform, test_transform


def iid_partition(indices, num_clients, seed):
    """
    IIDï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒï¼‰ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

    ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å‡ç­‰ã«åˆ†é…
    å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯å…¨ã‚¯ãƒ©ã‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãæŒã¤

    Args:
        indices: ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
        num_clients: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°
        seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
        å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
    """
    rng = np.random.default_rng(seed)
    shuffled = np.array(indices)
    rng.shuffle(shuffled)
    splits = np.array_split(shuffled, num_clients)
    return [split.tolist() for split in splits]


def dirichlet_partition(labels, num_clients, alpha, seed):
    """
    Dirichletåˆ†å¸ƒã‚’ä½¿ç”¨ã—ãŸnon-IIDï¼ˆéç‹¬ç«‹åŒåˆ†å¸ƒï¼‰ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

    å®Ÿä¸–ç•Œã®é€£åˆå­¦ç¿’ã‚’æ¨¡æ“¬ã™ã‚‹ãŸã‚ã€å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’åã‚‰ã›ã‚‹
    alphaãŒå°ã•ã„ã»ã©åã‚ŠãŒå¤§ãããªã‚‹ï¼ˆæ¥µç«¯ãªå ´åˆã€å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ç‰¹å®šã‚¯ãƒ©ã‚¹ã®ã¿æŒã¤ï¼‰

    ã€Dirichletåˆ†å¸ƒã¨ã¯ã€‘
    - ç¢ºç‡åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹åˆ†å¸ƒ
    - alphaãŒå°ã•ã„â†’ä¸å‡ä¸€ãªåˆ†å¸ƒï¼ˆä¸€éƒ¨ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«åã‚‹ï¼‰
    - alphaãŒå¤§ãã„â†’å‡ä¸€ãªåˆ†å¸ƒï¼ˆIIDã«è¿‘ã¥ãï¼‰

    Args:
        labels: å…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«é…åˆ—
        num_clients: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°
        alpha: Dirichletãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå°ã•ã„ã»ã©ä¸å‡ä¸€ï¼‰
        seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
        å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
    """
    rng = np.random.default_rng(seed)
    labels = np.array(labels)
    client_indices = [[] for _ in range(num_clients)]
    num_classes = int(labels.max()) + 1

    # ã‚¯ãƒ©ã‚¹ã”ã¨ã«å‡¦ç†
    for class_idx in range(num_classes):
        # ã“ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        class_mask = labels == class_idx
        class_indices = np.where(class_mask)[0]
        rng.shuffle(class_indices)

        # Dirichletåˆ†å¸ƒã§å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸ã®å‰²ã‚Šå½“ã¦æ¯”ç‡ã‚’æ±ºå®š
        proportions = rng.dirichlet(np.full(num_clients, alpha))
        proportions = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]
        splits = np.split(class_indices, proportions)

        # å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰²ã‚Šå½“ã¦
        for client_id, split in enumerate(splits):
            client_indices[client_id].extend(split.tolist())

    # å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    for client_id in range(num_clients):
        rng.shuffle(client_indices[client_id])

    return client_indices


class SimpleCIFARNet(nn.Module):
    """
    CIFAR-10ç”»åƒåˆ†é¡ç”¨ã®CNNãƒ¢ãƒ‡ãƒ«

    ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã€‘
    - å…¥åŠ›: 32x32x3ï¼ˆRGBç”»åƒï¼‰
    - ç‰¹å¾´æŠ½å‡ºéƒ¨ï¼ˆfeaturesï¼‰:
        * Convå±¤(32ch) â†’ BatchNorm â†’ ReLU â†’ Convå±¤(32ch) â†’ BatchNorm â†’ ReLU â†’ MaxPool
        * Convå±¤(64ch) â†’ BatchNorm â†’ ReLU â†’ Convå±¤(64ch) â†’ BatchNorm â†’ ReLU â†’ MaxPool
        * Dropout(0.3)
    - åˆ†é¡éƒ¨ï¼ˆclassifierï¼‰:
        * Flatten â†’ å…¨çµåˆå±¤(256) â†’ ReLU â†’ Dropout(0.4) â†’ å…¨çµåˆå±¤(10)
    - å‡ºåŠ›: 10ã‚¯ãƒ©ã‚¹ï¼ˆCIFAR-10ã®ã‚¯ãƒ©ã‚¹æ•°ï¼‰

    ã€CIFAR-10ã®ã‚¯ãƒ©ã‚¹ã€‘
    0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer,
    5: dog, 6: frog, 7: horse, 8: ship, 9: truck
    """

    def __init__(self):
        super(SimpleCIFARNet, self).__init__()

        # ç‰¹å¾´æŠ½å‡ºéƒ¨ï¼ˆç•³ã¿è¾¼ã¿å±¤ï¼‰
        self.features = nn.Sequential(
            # ç¬¬1ãƒ–ãƒ­ãƒƒã‚¯: 32x32x3 â†’ 32x32x32 â†’ 16x16x32
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 3ãƒãƒ£ãƒ³ãƒãƒ«â†’32ãƒãƒ£ãƒ³ãƒãƒ«
            nn.BatchNorm2d(32),  # ãƒãƒƒãƒæ­£è¦åŒ–ï¼ˆå­¦ç¿’ã‚’å®‰å®šåŒ–ï¼‰
            nn.ReLU(inplace=True),  # æ´»æ€§åŒ–é–¢æ•°
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 2x2ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆã‚µã‚¤ã‚ºåŠæ¸›ï¼‰

            # ç¬¬2ãƒ–ãƒ­ãƒƒã‚¯: 16x16x32 â†’ 16x16x64 â†’ 8x8x64
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 32ãƒãƒ£ãƒ³ãƒãƒ«â†’64ãƒãƒ£ãƒ³ãƒãƒ«
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 8x8ã¾ã§ç¸®å°
            nn.Dropout(0.3),  # éå­¦ç¿’é˜²æ­¢
        )

        # åˆ†é¡éƒ¨ï¼ˆå…¨çµåˆå±¤ï¼‰
        self.classifier = nn.Sequential(
            nn.Flatten(),  # 8x8x64 = 4096 â†’ 1æ¬¡å…ƒã«å¹³å¦åŒ–
            nn.Linear(64 * 8 * 8, 256),  # 4096 â†’ 256
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),  # éå­¦ç¿’é˜²æ­¢
            nn.Linear(256, 10),  # 256 â†’ 10ã‚¯ãƒ©ã‚¹ï¼ˆæœ€çµ‚å‡ºåŠ›ï¼‰
        )

    def forward(self, x):
        """
        é †ä¼æ’­å‡¦ç†

        Args:
            x: å…¥åŠ›ç”»åƒãƒ†ãƒ³ã‚½ãƒ« (batch_size, 3, 32, 32)

        Returns:
            å„ã‚¯ãƒ©ã‚¹ã®ã‚¹ã‚³ã‚¢ (batch_size, 10)
        """
        x = self.features(x)  # ç‰¹å¾´æŠ½å‡º
        return self.classifier(x)  # åˆ†é¡


def build_client_loaders(dataset, num_clients, batch_size, iid, alpha, seed):
    """å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    indices = list(range(len(dataset)))
    if iid:
        partitions = iid_partition(indices, num_clients, seed)
    else:
        partitions = dirichlet_partition(dataset.targets, num_clients, alpha, seed)

    client_loaders = []
    for partition in partitions:
        subset = Subset(dataset, partition)
        loader = DataLoader(
            subset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=torch.cuda.is_available(),
        )
        client_loaders.append(loader)
    return client_loaders


class Client:
    """
    é€£åˆå­¦ç¿’ã«ãŠã‘ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå‚åŠ è€…ï¼‰

    ã€å½¹å‰²ã€‘
    - æ‰‹å…ƒã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    - å­¦ç¿’å¾Œã®é‡ã¿ã‚’ã‚µãƒ¼ãƒã«é€ä¿¡
    - ã‚µãƒ¼ãƒã‹ã‚‰å—ã‘å–ã£ãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã§åˆæœŸåŒ–

    ã€é€£åˆå­¦ç¿’ã®æµã‚Œï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¦–ç‚¹ï¼‰ã€‘
    1. ã‚µãƒ¼ãƒã‹ã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å—ä¿¡
    2. è‡ªåˆ†ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    3. è¨“ç·´å¾Œã®é‡ã¿ã‚’ã‚µãƒ¼ãƒã«é€ä¿¡
    4. æ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã¾ã§å¾…æ©Ÿ
    """
    def __init__(self, client_id, train_loader, device, lr=0.01, momentum=0.9, weight_decay=5e-4):
        self.client_id = client_id  # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID
        self.device = device  # è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ï¼ˆCPU or GPUï¼‰
        self.train_loader = train_loader  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®DataLoader
        self.lr = lr  # å­¦ç¿’ç‡
        self.momentum = momentum  # SGDã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ 
        self.weight_decay = weight_decay  # é‡ã¿æ¸›è¡°ï¼ˆæ­£å‰‡åŒ–ï¼‰

    def local_update(self, global_weights, epochs=1):
        """
        ãƒ­ãƒ¼ã‚«ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã§æŒ‡å®šã‚¨ãƒãƒƒã‚¯æ•°ã ã‘å­¦ç¿’
        3. å­¦ç¿’å¾Œã®é‡ã¿ã¨ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¿”ã™

        Args:
            global_weights: ã‚µãƒ¼ãƒã‹ã‚‰å—ã‘å–ã£ãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿
            epochs: ãƒ­ãƒ¼ã‚«ãƒ«å­¦ç¿’ã®ã‚¨ãƒãƒƒã‚¯æ•°

        Returns:
            (cpu_state, total_samples): å­¦ç¿’å¾Œã®é‡ã¿ã¨ãƒ‡ãƒ¼ã‚¿æ•°
        """
        print(f"Client {self.client_id}: Starting local update")

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã§åˆæœŸåŒ–
        model = SimpleCIFARNet().to(self.device)
        model.load_state_dict(global_weights)
        model.train()  # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰

        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ï¼ˆSGD: ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼‰
        optimizer = optim.SGD(
            model.parameters(),
            lr=self.lr,
            momentum=self.momentum,  # ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã§åæŸã‚’å®‰å®šåŒ–
            weight_decay=self.weight_decay,  # L2æ­£å‰‡åŒ–
        )
        # æå¤±é–¢æ•°ï¼ˆã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®æ¨™æº–ï¼‰
        criterion = nn.CrossEntropyLoss()

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        total_loss = 0.0
        total_samples = 0
        for _ in range(epochs):
            for inputs, targets in self.train_loader:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)

                # å‹¾é…ã‚’ã‚¼ãƒ­ã‚¯ãƒªã‚¢
                optimizer.zero_grad(set_to_none=True)
                # é †ä¼æ’­
                outputs = model(inputs)
                # æå¤±è¨ˆç®—
                loss = criterion(outputs, targets)
                # é€†ä¼æ’­ï¼ˆå‹¾é…è¨ˆç®—ï¼‰
                loss.backward()
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
                optimizer.step()

                # çµ±è¨ˆæƒ…å ±ã®è¨˜éŒ²
                batch_size = inputs.size(0)
                total_loss += loss.item() * batch_size
                total_samples += batch_size

        avg_loss = total_loss / max(total_samples, 1)
        print(f"Client {self.client_id}: Completed training with loss={avg_loss:.4f}")

        # å­¦ç¿’å¾Œã®é‡ã¿ã‚’CPUã«ç§»å‹•ã—ã¦è¿”ã™ï¼ˆã‚µãƒ¼ãƒã¸é€ä¿¡ã™ã‚‹ãŸã‚ï¼‰
        cpu_state = {key: value.detach().cpu() for key, value in model.state_dict().items()}
        return cpu_state, total_samples


# =========================================================
# CKKSæš—å·åŒ–ã‚µãƒ¼ãƒ: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨æš—å·åŒ–é›†ç´„
# =========================================================
class FHEServer:
    """
    CKKSæº–åŒå‹æš—å·ã‚’ä½¿ç”¨ã™ã‚‹é€£åˆå­¦ç¿’ã‚µãƒ¼ãƒ

    ã€å½¹å‰²ã€‘
    - ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿æŒã¨è©•ä¾¡
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰å—ã‘å–ã£ãŸé‡ã¿ã‚’CKKSæš—å·åŒ–ã§é›†ç´„
    - ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ä¿è­·ã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°

    ã€é€£åˆå­¦ç¿’ã®æµã‚Œï¼ˆã‚µãƒ¼ãƒè¦–ç‚¹ï¼‰ã€‘
    1. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    2. å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ï¼š
       a. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é…å¸ƒ
       b. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰æ›´æ–°ã•ã‚ŒãŸé‡ã¿ã‚’å—ä¿¡
       c. CKKSæš—å·åŒ–ã§é‡ã¿ã‚’é›†ç´„ï¼ˆå¹³æ–‡ã‚’è¦‹ãšã«å¹³å‡åŒ–ï¼‰
       d. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
       e. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
    """
    def __init__(self, num_clients, test_loader, device):
        self.num_clients = num_clients
        self.device = device
        self.global_model = SimpleCIFARNet().to(self.device)

        # [REPLACED WITH OPENFHE-CKKS] ã“ã“ã§ CKKS ç‰ˆã® Aggregator ã‚’ä½¿ç”¨
        self.fhe_aggregator = FHEModelAggregator(
            self.global_model,
            num_clients=num_clients,
            scale_factor=100,   # å¼•æ•°äº’æ›ã®ãŸã‚å—ã‘æ¸¡ã—ã¯ã™ã‚‹ãŒã€CKKS ç‰ˆã§ã¯æœªä½¿ç”¨
            max_value=50        # åŒä¸Š
        )

        self.test_loader = test_loader
        self.criterion = nn.CrossEntropyLoss()

    def evaluate_global_model(self):
        """ã‚µãƒ¼ãƒã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡"""
        self.global_model.eval()
        correct = 0
        total = 0
        total_loss = 0.0
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.global_model(data)
                loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                correct += (predicted == target).sum().item()
                total += target.size(0)
                total_loss += loss.item()
        accuracy = correct / total * 100
        avg_loss = total_loss / len(self.test_loader)
        print(f"Global Model - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy, avg_loss

    def aggregate_models_with_fhe(self, client_updates):
        """
        CKKS ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’å¹³å‡ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¸é©ç”¨ã€‚
        client_updates: list of (state_dict, num_samples) tuples
        """
        print("ğŸ”’ FHE-based model aggregation (OpenFHE CKKS)...")
        # state_dictã®ã¿ã‚’æŠ½å‡º
        client_weights_list = [state_dict for state_dict, _ in client_updates]
        aggregated_weights = self.fhe_aggregator.aggregate_encrypted_models(client_weights_list)
        self.global_model.load_state_dict(aggregated_weights)
        print("ğŸ”’ FHE Global model updated")

    def get_global_weights(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é…å¸ƒã™ã‚‹ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’è¿”ã™"""
        return copy.deepcopy(self.global_model.state_dict())


# =========================================================
# å¹³æ–‡ã‚µãƒ¼ãƒ: æš—å·åŒ–ãªã—ã®é€šå¸¸ã®é€£åˆå­¦ç¿’
# =========================================================
class PlainServer:
    """
    å½¹å‰²:
      - ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿æŒã¨è©•ä¾¡
      - å¹³æ–‡ã§ã®ãƒ¢ãƒ‡ãƒ«é›†ç´„ï¼ˆå˜ç´”å¹³å‡ï¼‰
    """
    def __init__(self, num_clients, test_loader, device):
        self.num_clients = num_clients
        self.device = device
        self.global_model = SimpleCIFARNet().to(self.device)
        self.test_loader = test_loader
        self.criterion = nn.CrossEntropyLoss()

    def evaluate_global_model(self):
        """ã‚µãƒ¼ãƒã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡"""
        self.global_model.eval()
        correct = 0
        total = 0
        total_loss = 0.0
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.global_model(data)
                loss = self.criterion(output, target)
                predicted = torch.argmax(output, dim=1)
                correct += (predicted == target).sum().item()
                total += target.size(0)
                total_loss += loss.item()
        accuracy = correct / total * 100
        avg_loss = total_loss / len(self.test_loader)
        print(f"Global Model - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        return accuracy, avg_loss

    def aggregate_models_plain(self, client_updates):
        """
        å¹³æ–‡ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’å¹³å‡ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¸é©ç”¨ã€‚
        client_updates: list of (state_dict, num_samples) tuples
        """
        print("ğŸ“Š Plain model aggregation (weighted averaging)...")

        # ã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹åŠ é‡å¹³å‡
        total_samples = sum(num_samples for _, num_samples in client_updates)
        aggregated_weights = {}

        for layer_name in client_updates[0][0].keys():
            # æ•´æ•°å‹ã®ãƒãƒƒãƒ•ã‚¡ï¼ˆnum_batches_trackedãªã©ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            if client_updates[0][0][layer_name].dtype in [torch.int32, torch.int64, torch.long]:
                aggregated_weights[layer_name] = client_updates[0][0][layer_name].clone()
            else:
                # åŠ é‡å¹³å‡ã‚’è¨ˆç®—
                weighted_sum = torch.zeros_like(client_updates[0][0][layer_name])
                for state_dict, num_samples in client_updates:
                    weight = num_samples / total_samples
                    weighted_sum += state_dict[layer_name] * weight
                aggregated_weights[layer_name] = weighted_sum

        self.global_model.load_state_dict(aggregated_weights)
        print("ğŸ“Š Plain Global model updated")

    def get_global_weights(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é…å¸ƒã™ã‚‹ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡ã¿ã‚’è¿”ã™"""
        return copy.deepcopy(self.global_model.state_dict())


# =========================================================
# æ¯”è¼ƒå®Ÿé¨“é–¢æ•°
# =========================================================
def run_federated_learning_comparison(
    num_clients=10,
    num_rounds=10,
    batch_size=64,
    local_epochs=1,
    lr=0.01,
    iid=True,
    alpha=0.5,
    seed=42,
    data_dir='./data'
):
    """
    å¹³æ–‡ã¨CKKSæš—å·åŒ–ã®é€£åˆå­¦ç¿’ã‚’å®Ÿè¡Œã—ã€ç²¾åº¦ã¨æ™‚é–“ã‚’æ¯”è¼ƒ
    """
    print("="*80)
    print("ğŸ”¬ FEDERATED LEARNING COMPARISON: Plain vs CKKS Encrypted")
    print("="*80)

    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    train_transform, test_transform = build_transforms()
    train_dataset = datasets.CIFAR10(data_dir, train=True, download=True, transform=train_transform)
    test_dataset = datasets.CIFAR10(data_dir, train=False, download=True, transform=test_transform)

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    client_loaders = build_client_loaders(
        train_dataset,
        num_clients=num_clients,
        batch_size=batch_size,
        iid=iid,
        alpha=alpha,
        seed=seed,
    )

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=torch.cuda.is_available(),
    )

    # çµæœæ ¼ç´ç”¨
    results = {
        'plain': {'accuracy': [], 'time': []},
        'ckks': {'accuracy': [], 'time': [], 'key_gen_time': 0}
    }

    # =========================================================
    # 1. å¹³æ–‡ã§ã®é€£åˆå­¦ç¿’
    # =========================================================
    print("\n" + "="*80)
    print("ğŸ“Š PLAIN FEDERATED LEARNING")
    print("="*80)

    plain_server = PlainServer(num_clients=num_clients, test_loader=test_loader, device=device)
    plain_clients = [
        Client(client_id=i+1, train_loader=client_loaders[i], device=device, lr=lr)
        for i in range(num_clients)
    ]

    print("\nInitial Global Model Evaluation (Plain):")
    initial_accuracy_plain, _ = plain_server.evaluate_global_model()
    results['plain']['accuracy'].append(initial_accuracy_plain)

    for round_num in range(num_rounds):
        print(f"\n{'='*60}")
        print(f"ğŸ“Š PLAIN ROUND {round_num + 1}")
        print(f"{'='*60}")

        round_start = time.time()

        global_weights = plain_server.get_global_weights()
        client_updates = []

        for client in plain_clients:
            print(f"\n--- Client {client.client_id} Local Update ---")
            state_dict, num_samples = client.local_update(global_weights, epochs=local_epochs)
            client_updates.append((state_dict, num_samples))

        print(f"\n--- ğŸ“Š Plain Server Aggregation ---")
        plain_server.aggregate_models_plain(client_updates)

        round_end = time.time()
        round_time = round_end - round_start

        print(f"\nRound {round_num + 1} Global Model Evaluation (Plain):")
        current_accuracy, _ = plain_server.evaluate_global_model()

        results['plain']['accuracy'].append(current_accuracy)
        results['plain']['time'].append(round_time)

        print(f"ğŸ“Š Plain Round {round_num + 1} completed in {round_time:.2f} seconds!")

    # =========================================================
    # 2. CKKSæš—å·åŒ–ã§ã®é€£åˆå­¦ç¿’
    # =========================================================
    print("\n" + "="*80)
    print("ğŸ”’ CKKS ENCRYPTED FEDERATED LEARNING")
    print("="*80)

    # éµç”Ÿæˆæ™‚é–“ã‚’è¨ˆæ¸¬
    key_gen_start = time.time()
    ckks_server = FHEServer(num_clients=num_clients, test_loader=test_loader, device=device)
    key_gen_end = time.time()
    key_gen_time = key_gen_end - key_gen_start
    results['ckks']['key_gen_time'] = key_gen_time

    print(f"ğŸ”‘ Key generation time: {key_gen_time:.2f} seconds")

    ckks_clients = [
        Client(client_id=i+1, train_loader=client_loaders[i], device=device, lr=lr)
        for i in range(num_clients)
    ]

    print("\nInitial Global Model Evaluation (CKKS):")
    initial_accuracy_ckks, _ = ckks_server.evaluate_global_model()
    results['ckks']['accuracy'].append(initial_accuracy_ckks)

    for round_num in range(num_rounds):
        print(f"\n{'='*60}")
        print(f"ğŸ”’ CKKS ENCRYPTED ROUND {round_num + 1}")
        print(f"{'='*60}")

        round_start = time.time()

        global_weights = ckks_server.get_global_weights()
        client_updates = []

        for client in ckks_clients:
            print(f"\n--- Client {client.client_id} Local Update ---")
            state_dict, num_samples = client.local_update(global_weights, epochs=local_epochs)
            client_updates.append((state_dict, num_samples))

        print(f"\n--- ğŸ”’ CKKS Server Aggregation ---")
        ckks_server.aggregate_models_with_fhe(client_updates)

        round_end = time.time()
        round_time = round_end - round_start

        print(f"\nRound {round_num + 1} Global Model Evaluation (CKKS):")
        current_accuracy, _ = ckks_server.evaluate_global_model()

        results['ckks']['accuracy'].append(current_accuracy)
        results['ckks']['time'].append(round_time)

        print(f"ğŸ”’ CKKS Round {round_num + 1} completed in {round_time:.2f} seconds!")

    # è¨“ç·´ã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚‚è¿”ã™ï¼ˆè¦–è¦šçš„è¨¼æ˜ã®ãŸã‚ï¼‰
    return results, {'plain': plain_server, 'ckks': ckks_server}


def plot_comparison_results(results, num_rounds, num_clients):
    """
    æ¯”è¼ƒçµæœã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã¦ä¿å­˜
    """
    key_gen_time = results['ckks']['key_gen_time']

    # å›³ã®ä½œæˆ
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))

    # ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·ï¼ˆ0ã¯Initialã€1ä»¥é™ã¯å„ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
    rounds = list(range(num_rounds + 1))

    # =========================================================
    # 1. ç²¾åº¦ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
    # =========================================================
    ax1 = axes[0]
    ax1.plot(rounds, results['plain']['accuracy'], 'o-', label='Plain', linewidth=2, markersize=8)
    ax1.plot(rounds, results['ckks']['accuracy'], 's-', label='CKKS Encrypted', linewidth=2, markersize=8)
    ax1.set_xlabel('Round', fontsize=12)
    ax1.set_ylabel('Accuracy (%)', fontsize=12)
    ax1.set_title('Federated Learning Accuracy Comparison: Plain vs CKKS', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(rounds)
    ax1.set_xticklabels(['Initial'] + [f'R{i}' for i in range(1, num_rounds + 1)])

    # =========================================================
    # 2. å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
    # =========================================================
    ax2 = axes[1]
    round_nums = list(range(1, num_rounds + 1))

    bar_width = 0.35
    x = np.arange(len(round_nums))

    bars1 = ax2.bar(x - bar_width/2, results['plain']['time'], bar_width,
                    label='Plain', alpha=0.8)
    bars2 = ax2.bar(x + bar_width/2, results['ckks']['time'], bar_width,
                    label='CKKS Encrypted', alpha=0.8)

    ax2.set_xlabel('Round', fontsize=12)
    ax2.set_ylabel('Time (seconds)', fontsize=12)
    ax2.set_title('Federated Learning Execution Time Comparison: Plain vs CKKS', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels([f'Round {i}' for i in round_nums])
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3, axis='y')

    # éµç”Ÿæˆæ™‚é–“ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
    key_gen_text = f'CKKS Key Generation Time: {key_gen_time:.2f} seconds\n(Not included in round times)'
    ax2.text(0.02, 0.98, key_gen_text, transform=ax2.transAxes,
             fontsize=11, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()

    # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
    output_file = f'federated_learning_comparison_clients{num_clients}_rounds{num_rounds}.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š Comparison graph saved to: {output_file}")

    plt.close()


# =========================================================
# ğŸ¨ è¦–è¦šçš„è¨¼æ˜: ç”»åƒåˆ†é¡ãŒå®Ÿéš›ã«å‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜
# =========================================================

def visualize_sample_predictions(model, test_dataset, device, num_samples=16, output_file='sample_predictions.png'):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã—ã€äºˆæ¸¬çµæœã‚’å¯è¦–åŒ–

    ã€ç›®çš„ã€‘
    ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒãƒ€ãƒŸãƒ¼ã§ã¯ãªãã€å®Ÿéš›ã«ç”»åƒã‚’åˆ†é¡ã—ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜

    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        test_dataset: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        num_samples: è¡¨ç¤ºã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # CIFAR-10ã®ã‚¯ãƒ©ã‚¹å
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']

    model.eval()

    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    indices = np.random.choice(len(test_dataset), num_samples, replace=False)

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    fig.suptitle('ğŸ” Sample Image Classification Results (Proof of Real Classification)',
                 fontsize=14, fontweight='bold')

    for idx, ax in enumerate(axes.flat):
        if idx >= num_samples:
            break

        # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        img, true_label = test_dataset[indices[idx]]

        # äºˆæ¸¬
        with torch.no_grad():
            img_batch = img.unsqueeze(0).to(device)
            output = model(img_batch)
            pred_label = output.argmax(dim=1).item()
            confidence = torch.softmax(output, dim=1)[0][pred_label].item()

        # ç”»åƒã‚’æ­£è¦åŒ–è§£é™¤ã—ã¦è¡¨ç¤ºç”¨ã«å¤‰æ›
        img_display = img.cpu().numpy().transpose(1, 2, 0)
        mean = np.array([0.4914, 0.4822, 0.4465])
        std = np.array([0.2023, 0.1994, 0.2010])
        img_display = std * img_display + mean
        img_display = np.clip(img_display, 0, 1)

        # ç”»åƒã‚’è¡¨ç¤º
        ax.imshow(img_display)
        ax.axis('off')

        # æ­£è§£ãƒ»ä¸æ­£è§£ã§è‰²åˆ†ã‘
        color = 'green' if pred_label == true_label else 'red'
        title = f'True: {class_names[true_label]}\nPred: {class_names[pred_label]}\n({confidence*100:.1f}%)'
        ax.set_title(title, fontsize=9, color=color, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\nğŸ¨ Sample predictions visualization saved to: {output_file}")
    plt.close()


def visualize_confusion_matrix(model, test_loader, device, output_file='confusion_matrix.png'):
    """
    æ··åŒè¡Œåˆ—ã‚’ç”Ÿæˆã—ã¦å¯è¦–åŒ–

    ã€æ··åŒè¡Œåˆ—ã¨ã¯ã€‘
    ç¸¦è»¸ãŒçœŸã®ãƒ©ãƒ™ãƒ«ã€æ¨ªè»¸ãŒäºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®è¡Œåˆ—
    å¯¾è§’æˆåˆ†ãŒæ­£è§£ã€éå¯¾è§’æˆåˆ†ãŒèª¤åˆ†é¡ã‚’ç¤ºã™
    å®Ÿéš›ã«åˆ†é¡ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã®å¼·åŠ›ãªè¨¼æ‹ 

    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        test_loader: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # CIFAR-10ã®ã‚¯ãƒ©ã‚¹å
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']

    model.eval()
    all_preds = []
    all_labels = []

    print("\nğŸ” Generating confusion matrix...")

    # å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—
    cm = confusion_matrix(all_labels, all_preds)

    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Number of samples'})
    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
    ax.set_title('Confusion Matrix - Proof of Real Image Classification',
                 fontsize=14, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"âœ… Confusion matrix saved to: {output_file}")
    plt.close()

    return cm, all_labels, all_preds


def print_classification_report(all_labels, all_preds):
    """
    ã‚¯ãƒ©ã‚¹ã”ã¨ã®è©³ç´°ãªåˆ†é¡æ€§èƒ½ã‚’è¡¨ç¤º

    ã€è¡¨ç¤ºå†…å®¹ã€‘
    - Precisionï¼ˆé©åˆç‡ï¼‰: äºˆæ¸¬ã—ãŸä¸­ã§å®Ÿéš›ã«æ­£ã—ã‹ã£ãŸå‰²åˆ
    - Recallï¼ˆå†ç¾ç‡ï¼‰: å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹ã®ä¸­ã§æ­£ã—ãäºˆæ¸¬ã§ããŸå‰²åˆ
    - F1-score: Precisionã¨Recallã®èª¿å’Œå¹³å‡
    - Support: å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°

    Args:
        all_labels: çœŸã®ãƒ©ãƒ™ãƒ«
        all_preds: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
    """
    # CIFAR-10ã®ã‚¯ãƒ©ã‚¹å
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']

    print("\n" + "="*80)
    print("ğŸ“Š DETAILED CLASSIFICATION REPORT (Proof of Real Classification)")
    print("="*80)

    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report = classification_report(all_labels, all_preds,
                                   target_names=class_names,
                                   digits=4)
    print(report)

    # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ç²¾åº¦ã‚’è¨ˆç®—
    print("\n" + "="*80)
    print("ğŸ¯ Per-Class Accuracy")
    print("="*80)

    cm = confusion_matrix(all_labels, all_preds)
    for i, class_name in enumerate(class_names):
        class_acc = cm[i, i] / cm[i, :].sum() * 100 if cm[i, :].sum() > 0 else 0
        print(f"{class_name:12s}: {class_acc:6.2f}% ({cm[i, i]:4d}/{cm[i, :].sum():4d})")

    print("="*80)


def print_comparison_summary(results, num_rounds):
    """
    æ¯”è¼ƒçµæœã®ã‚µãƒãƒªãƒ¼ã‚’æ¨™æº–å‡ºåŠ›
    """
    print("\n" + "="*80)
    print("ğŸ“ˆ COMPARISON SUMMARY")
    print("="*80)

    print(f"\nğŸ”‘ CKKS Key Generation Time: {results['ckks']['key_gen_time']:.2f} seconds")
    print("   (This time is not included in round execution times)")

    print("\n" + "-"*80)
    print("ğŸ“Š Accuracy Comparison (per round):")
    print("-"*80)
    print(f"{'Round':<15} {'Plain (%)':<15} {'CKKS (%)':<15} {'Difference':<15}")
    print("-"*80)

    for i in range(num_rounds + 1):
        round_name = "Initial" if i == 0 else f"Round {i}"
        plain_acc = results['plain']['accuracy'][i]
        ckks_acc = results['ckks']['accuracy'][i]
        diff = ckks_acc - plain_acc
        print(f"{round_name:<15} {plain_acc:>8.2f}%      {ckks_acc:>8.2f}%      {diff:>+8.2f}%")

    print("\n" + "-"*80)
    print("â±ï¸  Execution Time Comparison (per round):")
    print("-"*80)
    print(f"{'Round':<15} {'Plain (s)':<15} {'CKKS (s)':<15} {'Overhead':<15}")
    print("-"*80)

    for i in range(num_rounds):
        plain_time = results['plain']['time'][i]
        ckks_time = results['ckks']['time'][i]
        overhead = ((ckks_time / plain_time) - 1) * 100
        print(f"Round {i+1:<8} {plain_time:>8.2f}s      {ckks_time:>8.2f}s      {overhead:>+8.1f}%")

    # å¹³å‡å€¤
    avg_plain_time = np.mean(results['plain']['time'])
    avg_ckks_time = np.mean(results['ckks']['time'])
    avg_overhead = ((avg_ckks_time / avg_plain_time) - 1) * 100

    print("-"*80)
    print(f"{'Average':<15} {avg_plain_time:>8.2f}s      {avg_ckks_time:>8.2f}s      {avg_overhead:>+8.1f}%")
    print("-"*80)

    print("\n" + "="*80)


# =========================================================
# å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# =========================================================
def main():
    """
    æ¯”è¼ƒå®Ÿé¨“ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    parser = argparse.ArgumentParser(
        description='Federated Learning Comparison: Plain vs CKKS Encrypted with CIFAR-10'
    )
    parser.add_argument('--clients', type=int, default=10, help='Number of clients (default: 10)')
    parser.add_argument('--rounds', type=int, default=10, help='Number of federated learning rounds (default: 10)')
    parser.add_argument('--batch-size', type=int, default=64, help='Batch size (default: 64)')
    parser.add_argument('--local-epochs', type=int, default=1, help='Local epochs per round (default: 1)')
    parser.add_argument('--lr', type=float, default=0.01, help='Learning rate (default: 0.01)')
    parser.add_argument('--iid', action='store_true', help='Use IID data partitioning (default: non-IID)')
    parser.add_argument('--alpha', type=float, default=0.5, help='Dirichlet alpha for non-IID (default: 0.5)')
    parser.add_argument('--seed', type=int, default=42, help='Random seed (default: 42)')
    parser.add_argument('--data-dir', type=str, default='./data', help='Data directory (default: ./data)')

    args = parser.parse_args()

    print(f"Starting federated learning with {args.clients} clients and {args.rounds} rounds")
    print(f"Batch size: {args.batch_size}, Local epochs: {args.local_epochs}, LR: {args.lr}")
    print(f"Data partitioning: {'IID' if args.iid else f'non-IID (alpha={args.alpha})'}")

    # æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ
    results, servers = run_federated_learning_comparison(
        num_clients=args.clients,
        num_rounds=args.rounds,
        batch_size=args.batch_size,
        local_epochs=args.local_epochs,
        lr=args.lr,
        iid=args.iid,
        alpha=args.alpha,
        seed=args.seed,
        data_dir=args.data_dir
    )

    # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    print_comparison_summary(results, args.rounds)

    # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    plot_comparison_results(results, args.rounds, args.clients)

    # =========================================================
    # ğŸ¨ è¦–è¦šçš„è¨¼æ˜: ç”»åƒåˆ†é¡ãŒå®Ÿéš›ã«å‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜
    # =========================================================
    print("\n" + "="*80)
    print("ğŸ¨ VISUAL PROOF: Demonstrating Real Image Classification")
    print("="*80)

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    _, test_transform = build_transforms()
    test_dataset = datasets.CIFAR10(args.data_dir, train=False, download=True, transform=test_transform)
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=torch.cuda.is_available(),
    )

    # =========================================================
    # å¹³æ–‡ã®é€£åˆå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§è¦–è¦šçš„è¨¼æ˜
    # =========================================================
    plain_server_final = servers['plain']

    print("\n" + "-"*80)
    print("ğŸ“Š PLAIN FEDERATED LEARNING - Visual Proof")
    print("-"*80)
    print(f"ğŸ“ Using the Plain trained model for visual proof.")
    print(f"   This model achieved {results['plain']['accuracy'][-1]:.2f}% accuracy on CIFAR-10.")

    # 1. ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®äºˆæ¸¬çµæœã‚’å¯è¦–åŒ–
    print("\nğŸ¨ Visualizing sample predictions (Plain)...")
    visualize_sample_predictions(
        plain_server_final.global_model,
        test_dataset,
        device,
        num_samples=16,
        output_file=f'sample_predictions_plain_clients{args.clients}.png'
    )

    # 2. æ··åŒè¡Œåˆ—ã‚’ç”Ÿæˆ
    print("\nğŸ“Š Generating confusion matrix (Plain)...")
    cm_plain, all_labels_plain, all_preds_plain = visualize_confusion_matrix(
        plain_server_final.global_model,
        test_loader,
        device,
        output_file=f'confusion_matrix_plain_clients{args.clients}.png'
    )

    # 3. è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    print("\nğŸ“Š Classification Report (Plain):")
    print_classification_report(all_labels_plain, all_preds_plain)

    # =========================================================
    # CKKSæš—å·åŒ–ã®é€£åˆå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§è¦–è¦šçš„è¨¼æ˜
    # =========================================================
    ckks_server_final = servers['ckks']

    print("\n" + "-"*80)
    print("ğŸ”’ CKKS ENCRYPTED FEDERATED LEARNING - Visual Proof")
    print("-"*80)
    print(f"ğŸ“ Using the CKKS-encrypted trained model for visual proof.")
    print(f"   This model achieved {results['ckks']['accuracy'][-1]:.2f}% accuracy on CIFAR-10.")

    # 1. ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®äºˆæ¸¬çµæœã‚’å¯è¦–åŒ–
    print("\nğŸ¨ Visualizing sample predictions (CKKS)...")
    visualize_sample_predictions(
        ckks_server_final.global_model,
        test_dataset,
        device,
        num_samples=16,
        output_file=f'sample_predictions_ckks_clients{args.clients}.png'
    )

    # 2. æ··åŒè¡Œåˆ—ã‚’ç”Ÿæˆ
    print("\nğŸ“Š Generating confusion matrix (CKKS)...")
    cm_ckks, all_labels_ckks, all_preds_ckks = visualize_confusion_matrix(
        ckks_server_final.global_model,
        test_loader,
        device,
        output_file=f'confusion_matrix_ckks_clients{args.clients}.png'
    )

    # 3. è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    print("\nğŸ“Š Classification Report (CKKS):")
    print_classification_report(all_labels_ckks, all_preds_ckks)

    print("\n" + "="*80)
    print("âœ… Visual proof completed for both Plain and CKKS!")
    print("="*80)
    print("\nğŸ“„ Generated files:")
    print(f"  1. federated_learning_comparison_clients{args.clients}_rounds{args.rounds}.png")
    print(f"  2. sample_predictions_plain_clients{args.clients}.png")
    print(f"  3. confusion_matrix_plain_clients{args.clients}.png")
    print(f"  4. sample_predictions_ckks_clients{args.clients}.png")
    print(f"  5. confusion_matrix_ckks_clients{args.clients}.png")
    print("\nThese visualizations prove that real image classification is being performed,")
    print("and allow comparison between Plain and CKKS encrypted federated learning!")

    print("\nâœ… All comparisons completed!")


if __name__ == "__main__":
    main()

